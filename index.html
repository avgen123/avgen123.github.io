<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Call for Papers: IJCV Special Issue</title>

    <!-- Meta tags for search engines to crawl -->
    <meta name="robots" content="index,follow">
    <meta name="description"
          content="Call for Papers: IJCV Special Issue"
          .
    >
    <meta name="keywords" content="Call for Papers: IJCV Special Issue">
    <link rel="author" href="https://scholar.google.com/citations?user=OQMtSKIAAAAJ&hl=en">

    <!-- Fonts and stuff -->
    <link href="./css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="./project.css" media="screen">
    <link rel="stylesheet" type="text/css" media="screen" href="./iconize.css">
    <script async="" src="./prettify.js"></script>

</head>

<body>
<div id="content">
    <div id="content-inner">

        <center><img src="./img/IJCV_LOGO-1.png" border="0" width="99.5%"><br>
            <!--            Left: The specification of the proposed 100-Driver for distracted driver classification. Right: The comparisons of different datasets.-->
        </center>

        <div class="section head">
            <h1> Call for Papers: IJCV Special Issue</h1>
            <h1> Audio-Visual Generation</h1>
        </div>


        <div class="section abstract">
            <h2>Introduction</h2>
            <br>
            <p>The ability to simulate and reason about the physical world is central to human intelligence. We perceive our surroundings and construct mental models that allow us to internally simulate possible outcomes, enabling reasoning, planning, and action — what we might call “world simulators”. Similarly, developing a world simulator is crucial for building human-like AI systems that can interact effectively with dynamic and complex environments. Recent research has shown that high-fidelity video generation models are a promising path toward building such comprehensive and efficient world simulators. However, the physical world is inherently multimodal. Human perception mostly relies not only on visual stimuli but also on sound. Sound often conveys critical information complementing what we can see, providing a richer and more nuanced understanding of the environment. To create world simulators capable of mimicking human-like perception and reasoning, it is crucial to develop coherent audiovisual generative models. Despite this, most modern approaches focus on vision-only or language-visual modalities, often with less focus on understanding and generating integrated audiovisual signals.</p>
            <p>This special issue aims to spotlight the exciting yet underexplored field of audio-visual generation as a key stepping stone towards achieving multi-modal world simulators. Our goal is to prioritize innovative approaches that explore this multimodal integration, advancing both the generation and analysis of audio-visual content. In addition to these approaches, we also aim to explore the broader impacts of this research. Moreover, in line with the classical concept of analysis-by-synthesis, advances in audiovisual generation can foster improvements in analysis and understanding methods, reinforcing the symbiotic relationship between these two areas. This research is not merely about content creation; it holds the potential to form a fundamental building block for more advanced, human-like AI systems.</p>
        </div>

        <div class="section abstract">
            <h2>Aims&Scope</h2>
            <br>
            <li><b>Audio and image/video generation.</b> Physically accurate and coherent audio-video generation models would be a promising way to build comprehensive and efficient world simulators. Audio-driven image/video generation or joint cogeneration is crucial for enabling more immersive and interactive experiences by synchronizing visual outputs with auditory inputs. This subtopic includes joint audio-visual generation, audio-to-image/video translation, and audio guided image/video editing, but not limited to these areas, as long as the techniques serve as promising foundational components.
            </li>
            <li><b>Audio-conditional X generation.</b> In this subtopic, we consider the audio-driven cross-modal generation of other various modalities beyond just image and video, including 3D/4D scenes, human motion (e.g., dance and gestures), and virtual avatars. Although successful cross-modal generation may seem straightforward, it is challenging because it requires accurately capturing the underlying associations between audio and the target modality.  This line of research broadens our understanding of the intersections and mutually exclusive information between different modalities. We are also interested in models where any-to-any generation can be achieved.</li>
            <li><b>Speech and talking avatar generation.</b> Speech is a unique type of audio signal compared to ambient environmental sounds, as it conveys both linguistic information and social cues. The speech and facial movements of a speaker are highly correlated. Generating talking faces, head movements, and gestures in 2D/3D not only enhances the realism and expressiveness of virtual characters in multimedia applications but also deepens our understanding of the relationship between linguistic information in speech and natural face/body movement, resulting in greater perceptual coherence. This will be a key world simulator that can help drive authentic human-machine interaction.
            </li>
            <li><b>Advanced audio-visual adaptor or interface.</b> Audio-visual generation involves understanding and extracting complex underlying information embedded in audio signals, and re-compiling that information into a completely different visual modality. This is often implemented by combining pre-trained modality encoders and decoders with simple, learnable adaptor modules. However, this complex process may not be effectively modeled by a simple stack of fully connected layers. In this subtopic, we consider the design of adaptors or interfaces that can effectively bridge the feature spaces of heterogeneous audio-visual modalities. The importance of these interfaces is highlighted in multi-modal large language models and any-to-any generation models as well.  We also encourage research that analyzes and improves our understanding of the modality gap between audio and visual representations.
            </li>
            <li><b>Benchmark and dataset.</b> One obvious obstacle in the research of audio-visual generation is the lack of high-quality data both for training and for benchmarking the performance of existing methods. In this subtopic, we welcome contributions that introduce new datasets and evaluation metrics, which are beneficial for advancing the field of audiovisual generation.
            </li>
            <li><b>Ethical considerations and social impact.</b> Ethical considerations in audio-visual generation research are crucial to ensuring the responsible use of technology, protecting individual privacy, and preventing potential misuse or harmful societal impacts, such as hallucination effects. In this subtopic, we welcome discussions on ethical and privacy concerns, such as algorithmic bias, AI-generated voice mimicry, and other potential risks associated with audiovisual generation.</li>
            <li><b>Generic topics and applications related to audio-visual generation.</b> We also welcome submissions on broader audio-visual research areas that can advance audio-visual generation. These include innovative model architectures, novel learning algorithms, dedicated loss functions, and improved data processing techniques (such as curation, filtering, and augmentation). Furthermore, research on fusion methods, conditioning mechanisms, and other methods that facilitate the integration of audio and visual modalities is encouraged. Beyond the topics mentioned, we invite research on how audio-visual generation models and their components can facilitate downstream applications, such as robotic learning, game design, and advertisement/film creation as a proxy of the world simulator.
            </li>
        </div>


        <div class="section abstract">
            <h2>Submission Guidelines</h2>
            <br>
            <p>Refer to the <a href="https://link.springer.com/journal/11263/updates/27726868">Official site</a></p>
            <div style="color: blue;">
            <h3>FQA</h2>
                <li><b>Q</b>: Will you accept conference extention? <b>A</b>: Yes. Conference-based extended papers are expected to have a minimum of 30% additional scientific contribution, e.g., in the form of new or improved algorithms or analysis, new experiments or qualitative/quantitative comparisons. As long as it is noted in the submission that the paper is extended from a conference paper, these should be fine.</li>
                <li><b>Q</b>: Will audio-visual learning (discriminative learning rather than generation) suitable for the special issue? <b>A</b>: Yes. We welcome submissions from relevant topics.</li>
            </div>
        </div>



        <div class="section abstract">
            <h2>Important Dates</h2>
            <br>
            <li>Manuscript submission deadline: 		<del>Mar. 15, 2025</del> <span style="color: red;">to be extended</span></li>
            <li>First review notification: 				<del>May 25, 2025</del> <span style="color: red;">to be extended</span></li>
            <li>Revised manuscript submission: 		Jul. 10, 2025</li>
            <li>Final review notification: 				Aug. 10, 2025</li>
            <li>Final manuscript submission: 			Sep. 20, 2025</li>
            <li>Publication year: 					2025</li>
        </div>


        <div class="section abstract">
            <h2>Guest Editors</h2>
            <br>
            <li>  <a href="https://ami.postech.ac.kr/members/tae-hyun-oh">Tae-Hyun Oh</a>: Pohang University of Science and Technology (POSTECH), South Korea, <a href = "mailto: thoh.kaist.ac.kr@gmail.com">thoh.kaist.ac.kr@gmail.com</a></li>

            <li>  <a href="https://www.shiqiyang.xyz/">Shiqi Yang</a>: SB Intuitions, SoftBank, Japan, <a href = "mailto: shiqi.yang@sbintuitions.co.jp">shiqi.yang@sbintuitions.co.jp</a></li>

            <li> <a href="https://lightchaserx.github.io/">Zhixiang Wang</a>: CyberAgent AI Lab, Japan, <a href = "mailto:  wangzx1994@gmail.com"> wangzx1994@gmail.com</a></li>

            <li>  <a href="https://stulyakov.com/">Sergey Tulyakov</a>: Snap Inc, US, <a href = "mailto: stulyakov@snap.com">stulyakov@snap.com</a></li>

            <li>  <a href="https://ibug.doc.ic.ac.uk/people/spetridis">Stavros Petridis</a>: Meta and Imperial College London, UK, <a href = "mailto: stavrosp@meta.com">stavrosp@meta.com</a></li>

            <li>  <a href="https://vicky.kalogeiton.info/">Vicky Kalogeiton</a>: Ecole Polytechnique, IP Paris, France, <a href = "mailto: vicky.kalogeiton@polytechnique.edu">vicky.kalogeiton@polytechnique.edu</a></li>

            <li>  <a href="http://faculty.ucmerced.edu/mhyang">Ming-Hsuan Yang</a>: University of California, Merced, US and Yonsei University, South Korea, <a href = "mailto: mhyang@ucmerced.edu">mhyang@ucmerced.edu</a></li>

        </div>

        <div class="section method">
            <h2>Concact</h2>
            <br>
            If you have any question related to this IJCV SI, please contact Tae-Hyun Oh, Shiqi Yang, or Zhixiang Wang.
        </div>

</body>
</html>
